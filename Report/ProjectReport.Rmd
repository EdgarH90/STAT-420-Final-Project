---
title: "Baseball Salary Analysis"
author: ""
date: "`r Sys.Date()`"
output:
  html_document:
    theme: readable
    toc: yes
  pdf_document:
    toc: yes
urlcolor: cyan
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

We will be examining what statistics related to a baseball player's performance have a significant effect on the salary of a player. This is particularly interesting as it is sort of the flip-side of the coin to the traditional Sabermetrics employed by the Oakland Athletics in the 1990's to analyze player statistics to try to assemble a winning team. We will be instead be leveraging the dataset to instead work for the players, hopefully determining which statistic(s) a player should focus on improving in order to increase their compensation.

Before we begin our analysis, we must first load the data and other dependencies from their respective libraries:
```{r warning=FALSE, include=TRUE, message= FALSE}
library(Lahman)
library(knitr)
library(lmtest)
library(scales)
library(ggplot2)
```

We then join the Salaries, Fielding, and Batting tables:
```{r warning=FALSE, include=FALSE}
# Joining relevant tables
tables = list(Salaries, Fielding, Batting)
salary_data = Reduce(function(x,y) merge(x,y, by= c("playerID", "yearID", "teamID","lgID")), tables)
# Cleaning up redundant columns
salary_data = subset(salary_data, subset = yearID > 1900,
                     select = -c(stint.x, stint.y, G.x, G.y, SB.x, SB.y, CS.x, CS.y, PB, WP, ZR, playerID, teamID))

# Converting character variables to factor
salary_data[sapply(salary_data, is.character)] = lapply(salary_data[sapply(salary_data, is.character)], as.factor)

# Cleaning up Nan & Inf values
salary_data[is.na(salary_data) | salary_data=="Inf"] = NA
```

Additionally, we will increment all of the numeric variables by one to facilitate log transformations for the predictors:
```{r}
salary_data[ , ! colnames(salary_data) %in% c("yearID", "lgID", "POS") ] = 
  salary_data[ , ! colnames(salary_data) %in% c("yearID", "lgID", "POS") ] + 1
```

## Methods

First, we visualize pairwise correlations between `Salary` and other variables:

- Calculating the correlations:
```{r warning = FALSE, message= FALSE}
salary_data_plot = as.data.frame(lapply(salary_data, as.integer))
salary_data_cor = cor(salary_data_plot)
```

- Plotting the relationships:
```{r warning = FALSE, message= FALSE}
library(ggcorrplot)
my_plt = ggcorrplot(salary_data_cor, lab = TRUE, lab_size = 1, type = "lower")
my_plt + theme(axis.text.x = element_text(size = 5), axis.text.y = element_text(size = 5))
```

- As expected, certain batting predictors such as `Doubles (X2B)` and `Triples (X3B)` have a very high correlation. We will keep these correlations under consideration as we refine our model.Additionally, we will remove `InnOuts` from our model since it is perfectly correlated with `Games Started (GS)`.

Before generating models, we begin by splitting the data into a test and train dataset. For this project we will split 60% of the data into a training set and 40% of the data into a testing set.

```{r}
salary_data = subset(salary_data, select = -c(InnOuts))
set.seed(20220805)
salary_trn_idx = sample(nrow(salary_data), size = trunc(0.60 * nrow(salary_data)))
salary_trn = salary_data[salary_trn_idx, ]
salary_tst = salary_data[-salary_trn_idx, ]
```

Next, we create a simple additive model with all of the predictors and a model with no predictors:
```{r}
simple_add = lm(salary ~ ., data = salary_trn)
simple_fit = lm(salary ~ 1, data = salary_trn)
```


We then use AIC and BIC stepwise procedures to refine our model ([See appendix](#appendix)):
```{r}
biggest = formula(lm(salary ~ ., data = salary_trn))
back_aic = step(simple_add, direction = "backward", trace = 0)

n = length(resid(simple_add))
back_bic = step(simple_add, direction = "backward", k = log(n), trace = 0)
```

- Now we can compare the LOOCV RMSE and Adjusted R Squared for both models:

```{r indent = "    "}
calc_loocv_rmse = function(model) {
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}
```

```{r indent = "    "}
(rsquared = c(summary(back_aic)$adj.r.squared, summary(back_bic)$adj.r.squared))

(rmse = c(calc_loocv_rmse(back_aic), calc_loocv_rmse(back_bic)))
```

- Although both models have similar Adjusted $R^2$ and LOOCV RMSE values, the BIC model uses fewer predictors. The AIC model contains `r length(attr(back_aic$terms, "term.labels"))` predictors and the BIC model contains `r length(attr(back_bic$terms, "term.labels"))` predictors. The predictors are as follows:

```{r indent = "    "}
attr(back_aic$terms, "term.labels")
attr(back_bic$terms, "term.labels")
```

However, it is likely that salary does not increase linearly in baseball and can range in orders of magnitude. Therefore, we will create a new additive model with a log transformation applied to our predictor to see if we can obtain a higher Adjusted $R^2$ and a lower LOOCV RMSE.
```{r}
log_fit = lm(log(salary) ~ ., data = salary_trn)
```

Next, we will perform the stepwise procedures:
```{r}
biggest = formula(lm(log(salary) ~ ., salary > 0, data = salary_trn))
back_aic_log = step(log_fit, direction = "backward", trace = 0)

n = length(resid(log_fit))
back_bic_log = step(log_fit, direction = "backward", k = log(n), trace = 0)
```

- Now we can compare the LOOCV RMSE and Adjusted R Squared for both models:

```{r indent = "    "}
calc_loocv_rmse_log = function(model) {
  sqrt(mean((exp(resid(model)) / (1 - exp(hatvalues(model)))) ^ 2))
}
```

```{r indent = "    "}
(rsquared_log_selected = c(summary(back_aic_log)$adj.r.squared, summary(back_bic_log)$adj.r.squared))
(rmse_log_selected = c(calc_loocv_rmse_log(back_aic_log), calc_loocv_rmse_log(back_bic_log)))
```

- Both models have a similar Adjusted $R^2$. Even though the AIC model has a lower LOOCV RMSE, the BIC model is significantly smaller. The AIC model contains `r length(attr(back_aic_log$terms, "term.labels"))` predictors and the BIC model contains `r length(attr(back_bic_log$terms, "term.labels"))` predictors. The predictors are as follows:

```{r indent = "    "}
(aic_terms = sort(attr(back_aic_log$terms, "term.labels")))
(bic_terms = sort(attr(back_bic_log$terms, "term.labels")))
```

- Since the BIC model contains a subset of predictors from our AIC model, we can run an $F$-Test for comparison. 
```{r indent = "    "}
(null_terms = aic_terms[which(!aic_terms %in% bic_terms)])
```

- Using $H_0: \beta_{A} = \beta_{DP} = \beta_{HBP} = \beta_{SH} = \beta_{SF} = 0$, we will determine if one of the $\beta_j$ from our null is not $0$:

```{r indent = "    "}
anova(back_bic_log, back_aic_log)
```

- Since we have an extremely small p-value, we reject the null at any reasonable $\alpha$ and say that at least one of the $\beta_j$ from our null is not $0$.

We can then run some diagnostics to ensure that our selected model does not violate normality and homoscedasticity assumptions.
```{r}
diagnostics = function(model, pcol = 'grey', lcol = 'dodgerblue',
                       alpha = .05, plotit = TRUE, testit = TRUE){
  
  if (plotit){
    par(mfrow = c(1, 2))
    plot(fitted(model), resid(model), col = pcol, pch = 20, 
         xlab = "Fitted", ylab = "Residuals", main = "Data from Model")
    abline(h = 0, col = lcol, lwd = 1)
    qqnorm(resid(model), main = "Normal Q-Q Plot, Model", col = pcol)
    qqline(resid(model), col = lcol, lwd = 1)
    
  }
  if (testit){
    p_val = shapiro.test(resid(model))$p.value
    decision = ifelse(p_val < alpha, "Reject", "Fail to Reject")
    return(list("p_val" = p_val, "decision" = decision))
  }
}
```

- Based on the plots below below, we can observe that our model currently violates both normality and equal variance assumptions:
```{r indent = "    "}
diagnostics(back_aic_log, testit = FALSE)
```

- We can attempt to stabilize the variance by applying log transformations to some of the predictors: 
```{r indent = "    "}
#selected_fit = lm(log(salary) ~ yearID + POS + log(GS) + log(PO) + log(E) + log(AB) +
                    #log(H) + log(X2B) + log(X3B) + log(HR) + log(RBI) + log(BB) +
                    #log(SO) + log(IBB) + log(GIDP), data = salary_trn)
selected_fit = lm(log(salary) ~ yearID + POS + log(GS) + log(PO) + log(E) + log(AB) +
                    log(H) + log(X2B) + log(X3B) + log(HR) + log(RBI) + log(BB) +
                    log(SO) + log(IBB) + log(GIDP) + log(A) + log(DP) + log(HBP) + log(SH) + log(SF), data = salary_trn)
```

- After running the diagnostics again, we can see that the variance looks better, but we still have quite a few issues with unusual observations:
```{r indent = "    "}
diagnostics(selected_fit, testit = FALSE)
```

- We can further refine our model by removing influential outliers. We will use the $D_i > \frac{4}{n}$ Cook's distance heuristic to identify them:
```{r indent = "    "}
cd_selected = cooks.distance(selected_fit)
large_cd  = cd_selected > 4 / length(cd_selected)

selected_fit_fix = lm(log(salary) ~ yearID + POS + log(GS) + log(PO) + log(E) + log(AB) +
                    log(H) + log(X2B) + log(X3B) + log(HR) + log(RBI) + log(BB) +
                    log(SO) + log(IBB) + log(GIDP), 
                  subset = -which(large_cd), data = salary_trn)
diagnostics(selected_fit_fix, testit = FALSE)
```

While the model above is looking much better, we will shift our focus on transforming only Fielding and Batting variables in isolation to see if we can improve our Adjusted $R^2$ amd LOOCV RMSE.

We first apply log transformations to the full additive model to variables regarding only batting statistics. Additionally, we will add the interactions between `Year` and `Position`. Finally, we will remove influential outliers to improve our model.

```{r}
log_fit_batting_selected = lm(log(salary) ~  + yearID * POS * (A + DP + E + GIDP + GS + lgID + log(AB) + log(BB) + log(GIDP) + log(H) + log(HBP) + log(HR) + log(IBB) + log(R) + log(RBI) + log(SF) + log(SH) + log(SO) + log(X2B) + log(X3B) + PO), subset = -which(large_cd), data = salary_trn)
bptest(log_fit_batting_selected)
diagnostics(log_fit_batting_selected, testit = FALSE)
```

When we perform the BP test we get a low p-value showing constant variance assumption is violated which matches our findings with the fitted versus residuals plot. When we look at the Q-Q plot it remains similar to the above model where there are still discrepancies.

Likewise, we can apply log transformations to variables regarding fielding statistics to the full additive model and add the `Year` and `POS` interactions.

```{r}
#log_fit_fielding_selected = lm(log(salary) ~ log(GS) + log(PO) + log(A) + log(E) + log(DP) + . - GS - PO - A - E - DP, subset = -which(large_cd), data = salary_trn)
log_fit_fielding_selected = lm(log(salary) ~ yearID * POS * (AB + BB + GIDP + H + HBP + HR + IBB + lgID + log(A) + log(DP) + log(E) + log(GS) + log(PO) + R + RBI + SF + SH + SO + X2B + X3B), subset = -which(large_cd), data = salary_trn)
bptest(log_fit_fielding_selected)
diagnostics(log_fit_fielding_selected, testit = FALSE)
```

We can see adjusting the predictors with log transformations did not make much difference to the assumption violations.

Now, we will compare the adjusted $R^2$ and LOOCV RMSE between our previously chosen AIC model and the two models above.

```{r}
(log_transform_rmse = c(calc_loocv_rmse_log(selected_fit_fix), calc_loocv_rmse_log(log_fit_batting_selected), calc_loocv_rmse_log(log_fit_fielding_selected)))

(log_transform_rsquared = c(summary(log_fit)$adj.r.squared, summary(log_fit_batting_selected)$adj.r.squared, summary(log_fit_fielding_selected)$adj.r.squared))

```

While the base AIC log model has a smaller $R^2$, both selected log models have a similar adjusted $R^2$ that is slightly larger. When looking at the LOOCV RMSE, the fielding selected model has a much lower LOOCV RMSE than the batting model.

Now we can use our test data to compare the average percent error between all the models:

```{r indent = "  "}
# Base Selected models
predict_back_aic = predict(back_aic, newdata = salary_tst)
predict_back_bic = predict(back_bic, newdata = salary_tst)

# Log Selected Models
predict_select_aic = exp(predict(back_aic_log, newdata = salary_tst))
predict_select_bic = exp(predict(back_bic_log, newdata = salary_tst))

# Transformed Models
predict_aic = exp(predict(selected_fit_fix, newdata = salary_tst))
predict_batting = exp(predict(log_fit_batting_selected, newdata = salary_tst))
predict_fielding = exp(predict(log_fit_fielding_selected, newdata = salary_tst))
n = length(predict_aic)

avg_pct_error_back_aic = (sum(abs(predict_back_aic - salary_tst$salary) / predict_back_aic) / n) * 100
avg_pct_error_back_bic = (sum(abs(predict_back_bic - salary_tst$salary) / predict_back_bic) / n) * 100

avg_pct_error_select_aic = (sum(abs(predict_select_aic - salary_tst$salary) / predict_select_aic) / n) * 100
avg_pct_error_select_bic = (sum(abs(predict_select_bic - salary_tst$salary) / predict_select_bic) / n) * 100

avg_pct_error_aic = (sum(abs(predict_aic - salary_tst$salary) / predict_aic) / n) * 100
avg_pct_error_bat = (sum(abs(predict_batting - salary_tst$salary) / predict_batting) / n) * 100
avg_pct_error_field = (sum(abs(predict_fielding - salary_tst$salary) / predict_fielding) / n) * 100

```

We can also calculate the test RMSE for our models:

```{r}
back_aic_rmse = sqrt(mean((predict_back_aic - salary_tst$salary)^2))
back_bic_rmse = sqrt(mean((predict_back_bic - salary_tst$salary)^2))

select_aic_rmse = sqrt(mean((predict_select_aic - salary_tst$salary)^2))
select_bic_rmse = sqrt(mean((predict_select_bic - salary_tst$salary)^2))

aic_rmse = sqrt(mean((predict_aic - salary_tst$salary)^2))
batting_rmse = sqrt(mean((predict_batting - salary_tst$salary)^2))
fielding_rmse = sqrt(mean((predict_fielding - salary_tst$salary)^2))
```


## Results


### Backwards AIC vs Backwards BIC

```{r }
aic_results = data.frame(Model = c("Backward AIC", "Backward BIC"),
                         AdjR = rsquared, RMSE = rmse)

kable(aic_results, 
      col.names = c("Selection Procedure", "Adjusted R. Squared", "LOOCV RMSE"))
```


### Log Selected Step Models

```{r}
aic_results_log_selected = data.frame(Model = c("Log Backward AIC", "Log Backward BIC"),
                         AdjR = rsquared_log_selected, RMSE = rmse_log_selected)

kable(aic_results_log_selected, 
      col.names = c("Selection Procedure", "Adjusted R. Squared", "LOOCV RMSE"))
```


### Transformed Models

```{r}
log_transform_results = data.frame(Model = c("Back AIC Selected Log Model", "Batting Selected Log Model", "Fielding Selected Log Model"), AdjR = log_transform_rsquared, RMSE = log_transform_rmse)

kable(log_transform_results, 
      col.names = c("Model", "Adjusted R. Squared", "LOOCV RMSE"))
```


### Test Results

For each set, we selected the model with the best average percent error and plotted our predicted salaries vs the actual test set salaries.

```{r}
avg_percent_error = c(avg_pct_error_back_aic, avg_pct_error_back_bic, avg_pct_error_select_aic, avg_pct_error_select_bic, avg_pct_error_aic, avg_pct_error_bat, avg_pct_error_field)

test_rmse = c(back_aic_rmse, back_bic_rmse, select_aic_rmse, select_bic_rmse, aic_rmse, batting_rmse, fielding_rmse)

test_set_results = data.frame(Model = c("Backward AIC", "Backward BIC", "Log Backward AIC", "Log Backward BIC", "Back AIC Selected Log Model", "Batting Selected Log Model", "Fielding Selected Log Model"), t_rmse = test_rmse, error = avg_percent_error)

kable(test_set_results, 
      col.names = c("Model", "Test RMSE", "Average Percent Error"))
```

```{r indent = "  "}
ggplot(data = data.frame(predict_back_aic, salary_tst$salary), mapping = aes(x = predict_back_aic, y = salary_tst$salary)) + 
  labs(x = "Predicted Salary (in Millions of $)") + 
  labs(y = "Actual Salary (in Millions of $)") + 
  labs(title = "Actual vs Predicted Salary - Backward AIC") + 
  geom_point(color = "darkblue", size = 1) + 
  scale_y_continuous(labels = label_number(suffix = " M", scale = 1e-6)) +
  scale_x_continuous(labels = label_number(suffix = " M", scale = 1e-6)) +
  geom_abline(slope = 1, intercept = 0, color = "red")
```

```{r indent = "  "}
ggplot(data = data.frame(predict_select_aic, salary_tst$salary), mapping = aes(x = predict_select_aic, y = salary_tst$salary)) + 
  labs(x = "Predicted Salary (in Millions of $)") + 
  labs(y = "Actual Salary (in Millions of $)") + 
  labs(title = "Actual vs Predicted Salary - Log Backward AIC") + 
  geom_point(color = "darkblue", size = 1) + 
  scale_y_continuous(labels = label_number(suffix = " M", scale = 1e-6)) +
  scale_x_continuous(labels = label_number(suffix = " M", scale = 1e-6)) +
  geom_abline(slope = 1, intercept = 0, color = "red")
```

```{r indent = "  "}
ggplot(data = data.frame(predict_batting, salary_tst$salary), mapping = aes(x = predict_batting, y = salary_tst$salary)) + 
  labs(x = "Predicted Salary (in Millions of $)") + 
  labs(y = "Actual Salary (in Millions of $)") + 
  labs(title = "Actual vs Predicted Salary - Batting Selected Log Model") + 
  geom_point(color = "darkblue", size = 1) + 
  scale_y_continuous(labels = label_number(suffix = " M", scale = 1e-6)) +
  scale_x_continuous(labels = label_number(suffix = " M", scale = 1e-6)) +
  geom_abline(slope = 1, intercept = 0, color = "red")
```

## Discussion

- Based on our results, it appears that player statistics cannot accurately predict player salaries. However, this was to be expected given the low Adjusted $R^2$ that we were able to achieve. Various external factors such as inflation, players' strike (lockout), and changes in collective bargaining agreements also play a role in determining salaries.

- Even though all of our error rates are extremely high, we still managed to decrease them substantially by refining the models. 

- Our models appear to be overfitting the data given the extremely large RMSEs for our test sets. For future analysis, we would need to simplify our selected model.

## Appendix {#appendix}

Team Members: Edgar Hernandez, Ryan Sims, Jessica Tomas

We also created a forward stepwise AIC model, which was the same size as the backward model:
```{r}
forw_aic = step(simple_fit, direction = "forward", scope = biggest, trace = 0)
summary(forw_aic)$adj.r.squared
calc_loocv_rmse(forw_aic)
all.equal(sort(attr(back_aic$terms, "term.labels")), sort(attr(forw_aic$terms, "term.labels")))
```

Additionally, we took a look at an additive model that takes into account all 2-way interactions and then perform backwards stepwise AIC against that fitted model to arrive at a set of significant coefficients. However, the model took multiple hours to compute and did not achieve desirable results given the extremely large number of variables.

```{r eval=FALSE}
system.time({
  slr3=lm(salary~(.)^2,salary_trn)
})
system.time({
  slr3_AIC=step(slr3,direction='backward', trace=0)
})
summary(slr3)
summary(slr3_AIC)
```

```{r eval=FALSE, indent="   "}
rsquared3 = c(summary(slr3_AIC)$adj.r.squared, summary(slr3)$adj.r.squared)
rmse3 = c(calc_loocv_rmse(slr3_AIC), calc_loocv_rmse(slr3))
aic_results3 = data.frame(Model = c("Backward AIC", "Full Additive with 2-way Interactions"),
                          AdjR = rsquared3, RMSE = rmse3)

kable(aic_results3, 
      col.names = c("Selection Procedure", "Adjusted R. Squared", "LOOCV RMSE"))
```

We also analyzed log transformations of the predictors without any interactions first:
```{r eval=FALSE}
log_fit_batting_selected = lm(log(salary) ~ log(AB) + log(R) + log(H) + log(X2B) + log(X3B) + log(HR) + log(RBI) + log(BB) + log(SO) + log(IBB) + log(HBP) + log(SH) + log(SF) + log(GIDP) + . - AB - R - H - X2B - X3B - RBI - BB - SO - IBB - HBP - SH - SF, subset = -which(large_cd), data = salary_trn)
```

